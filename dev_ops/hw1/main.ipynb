{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект-ДЗ по Эксплуатации\n",
    "\n",
    "В данном проекте вы выполните комплексное домашнее задание по подготовке, оптимизации и развертыванию модели машинного обучения с использованием современных инструментов. Цель проекта – освоить процессы обучения, конвертации, оптимизации и интеграции моделей в продакшн-среду с применением Triton Inference Server, Docker и микросервисной архитектуры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m venv .venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Обучение модели**\n",
    "   - Обучите самописную модель на базе **Torch** или **TensorFlow**.\n",
    "   - Используйте стандартные слои, избегая кастомных решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-win_amd64.whl (204.2 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting sympy==1.13.1\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Installing collected packages: mpmath, MarkupSafe, sympy, networkx, jinja2, fsspec, filelock, torch, pillow, numpy, torchvision\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.3 pillow-11.1.0 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\GitHub\\HSE\\dev_ops\\hw1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Среднее и стандартное отклонение MNIST\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU() \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc = nn.Linear(64 * 28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = x.view(x.shape[0], -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 16 17:35:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.70                 Driver Version: 572.70         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   36C    P8             35W /  320W |    1607MiB /  10240MiB |     28%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2432    C+G   ...Telegram Desktop\\Telegram.exe      N/A      |\n",
      "|    0   N/A  N/A            7072    C+G   ...ogram Files\\Karing\\karing.exe      N/A      |\n",
      "|    0   N/A  N/A            9676    C+G   ...owser\\Application\\browser.exe      N/A      |\n",
      "|    0   N/A  N/A           11652    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           13144    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           13164    C+G   ...us-librarian\\OVRLibrarian.exe      N/A      |\n",
      "|    0   N/A  N/A           13240    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           13516    C+G   ...se 3 Host\\Razer Synapse 3.exe      N/A      |\n",
      "|    0   N/A  N/A           14408    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           15900    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           18320    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18344    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           18536    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18568    C+G   ...4.49\\Signal-x64\\SignalRgb.exe      N/A      |\n",
      "|    0   N/A  N/A           20148    C+G   ...ef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A           26872    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           27476    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "|    0   N/A  N/A           30464    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           30472    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           31296    C+G   ...YandexMusic\\������ ������.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch as it is not installed.\n",
      "WARNING: Skipping torchvision as it is not installed.\n",
      "WARNING: Skipping torchaudio as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall torch torchvision torchaudio --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"-m\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
     ]
    }
   ],
   "source": [
    "! -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp310-cp310-win_amd64.whl (2496.1 MB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp310-cp310-win_amd64.whl (6.1 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "Requirement already satisfied: sympy==1.13.1 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: fsspec in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: networkx in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: filelock in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: jinja2 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: numpy in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.6.0+cu126 torchaudio-2.6.0+cu126 torchvision-0.21.0+cu126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\GitHub\\HSE\\dev_ops\\hw1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Получаем индекс максимального логита\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "MODEL_NAME = \"mnist_cnn.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000] Loss: 2.312083\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.140889\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.091742\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.129100\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.047927\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.039252\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.036285\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.011841\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.011470\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.060897\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9848/10000 (98.48%)\n",
      "\n",
      "Train Epoch: 2 [0/60000] Loss: 0.012237\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.053163\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.007935\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.023361\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.075113\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.010290\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.011733\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.133325\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.005644\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.012428\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9861/10000 (98.61%)\n",
      "\n",
      "Train Epoch: 3 [0/60000] Loss: 0.023614\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.002946\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.012913\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.049683\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.004803\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.002257\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.046803\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.030947\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.018822\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.074651\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9828/10000 (98.28%)\n",
      "\n",
      "Train Epoch: 4 [0/60000] Loss: 0.003808\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.001925\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.000869\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.001442\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.000391\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.000167\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.006644\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.001624\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.018599\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.142477\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9858/10000 (98.58%)\n",
      "\n",
      "Train Epoch: 5 [0/60000] Loss: 0.015610\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.007662\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.005890\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.017915\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.000515\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.001720\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.012526\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.000387\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.005561\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.001808\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9845/10000 (98.45%)\n",
      "\n",
      "Модель сохранена в mnist_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, test_loader, criterion)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_NAME)\n",
    "print(f\"Модель сохранена в {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN() \n",
    "model.load_state_dict(torch.load(MODEL_NAME))\n",
    "model.eval()\n",
    "\n",
    "traced_model = torch.jit.trace(model, torch.randn(1, 1, 28, 28))\n",
    "\n",
    "traced_model.save(\"model_ts.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Конвертация в ONNX**\n",
    "   - Экспортируйте обученную модель в формат **ONNX**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx==1.13.1 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnx==1.13.1) (2.2.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.20.2 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnx==1.13.1) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnx==1.13.1) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\GitHub\\HSE\\dev_ops\\hw1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install onnx==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime==1.14.1\n",
      "  Downloading onnxruntime-1.14.1-cp310-cp310-win_amd64.whl (6.5 MB)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: protobuf in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnxruntime==1.14.1) (3.20.3)\n",
      "Requirement already satisfied: packaging in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnxruntime==1.14.1) (24.2)\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnxruntime==1.14.1) (2.2.3)\n",
      "Requirement already satisfied: sympy in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnxruntime==1.14.1) (1.13.1)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from sympy->onnxruntime==1.14.1) (1.3.0)\n",
      "Installing collected packages: pyreadline3, humanfriendly, flatbuffers, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-25.2.10 humanfriendly-10.0 onnxruntime-1.14.1 pyreadline3-3.5.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\GitHub\\HSE\\dev_ops\\hw1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install onnxruntime==1.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.2.3\n",
      "Uninstalling numpy-2.2.3:\n",
      "  Successfully uninstalled numpy-2.2.3\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\GitHub\\HSE\\dev_ops\\hw1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успешно экспортирована в mnist_cnn.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "\n",
    "model = CNN() \n",
    "model.load_state_dict(torch.load(MODEL_NAME))\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 28, 28)  # (batch=1, channels=1, height=28, width=28)\n",
    "\n",
    "onnx_filename = \"mnist_cnn.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,  \n",
    "    dummy_input,\n",
    "    onnx_filename,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"], \n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}, \n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "print(f\"Модель успешно экспортирована в {onnx_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выход ONNX модели: [array([[-23.250158 , -13.814761 , -10.290759 ,  -8.994146 , -19.839657 ,\n",
      "         -8.587656 , -10.643445 , -10.049277 , -16.82718  , -14.6214905]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "onnx_model = onnx.load(\"mnist_cnn.onnx\")\n",
    "onnx.checker.check_model(onnx_model)  # Проверяем модель\n",
    "\n",
    "ort_session = ort.InferenceSession(\"mnist_cnn.onnx\")\n",
    "\n",
    "x = np.random.randn(1, 1, 28, 28).astype(np.float32)\n",
    "ort_inputs = {\"input\": x}\n",
    "ort_outs = ort_session.run([\"output\"], ort_inputs)\n",
    "\n",
    "print(f\"Выход ONNX модели: {ort_outs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Опционально) Конвертация в TensorRT (TRT)**\n",
    "   - При необходимости, конвертируйте модель в формат **TensorRT** для повышения производительности инференса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Оптимизация модели средствами Torch/TensorFlow**\n",
    "   - Примените встроенные методы оптимизации (например, quantization или pruning) для улучшения эффективности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN()\n",
    "# model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "# model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "# model_fused = torch.ao.quantization.fuse_modules(model, [['conv1', 'relu1'], ['conv2', 'relu2']])\n",
    "# model_prepared = torch.ao.quantization.prepare(model_fused)\n",
    "# model_int8 = torch.ao.quantization.convert(model_prepared)\n",
    "\n",
    "# torch.save(model_int8.state_dict(), \"mnist_cnn_quantized.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n",
    "model.eval()\n",
    "\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "model_fused = torch.ao.quantization.fuse_modules(model, [['conv1', 'relu1'], ['conv2', 'relu2']])\n",
    "model_prepared = torch.ao.quantization.prepare(model_fused)\n",
    "model_int8 = torch.ao.quantization.convert(model_prepared)\n",
    "\n",
    "scripted_model = torch.jit.script(model_int8)\n",
    "scripted_model.save(\"mnist_cnn_quantized_ts.pt\")  # TorchScript-модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.ao.quantization as quantization\n",
    "\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pth\"))\n",
    "model.eval()\n",
    "\n",
    "model.qconfig = quantization.QConfig(\n",
    "    activation=quantization.default_observer,\n",
    "    weight=quantization.default_weight_observer\n",
    ")\n",
    "\n",
    "model_fused = torch.ao.quantization.fuse_modules(model, [['conv1', 'relu1'], ['conv2', 'relu2']])\n",
    "model_prepared = torch.ao.quantization.prepare(model_fused)\n",
    "model_int8 = torch.ao.quantization.convert(model_prepared)\n",
    "\n",
    "scripted_model = torch.jit.script(model_int8)\n",
    "scripted_model.save(\"mnist_cnn_quantized_ts.pt\")  # TorchScript-модель\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Оптимизация модели инструментами ONNX и (опционально) TRT**\n",
    "   - Используйте оптимизирующие инструменты для ONNX (например, ONNX Runtime) для повышения производительности.\n",
    "   - (Опционально) Оптимизируйте модель в формате TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxoptimizer\n",
      "  Downloading onnxoptimizer-0.3.13-cp310-cp310-win_amd64.whl (381 kB)\n",
      "Requirement already satisfied: onnx in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnxoptimizer) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnx->onnxoptimizer) (4.12.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.20.2 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnx->onnxoptimizer) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.16.6 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from onnx->onnxoptimizer) (1.26.4)\n",
      "Installing collected packages: onnxoptimizer\n",
      "Successfully installed onnxoptimizer-0.3.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\GitHub\\HSE\\dev_ops\\hw1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip3 install onnxoptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступные бэкенды: ['CPUExecutionProvider']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-15.927749 , -11.864381 ,  -7.2300553, -10.10805  , -13.666244 ,\n",
       "         -10.109586 , -10.94195  , -12.203027 , -15.49283  , -13.321471 ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxoptimizer\n",
    "\n",
    "onnx_model_path = \"mnist_cnn.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "passes = [\n",
    "    \"eliminate_deadend\",        # Убирает ненужные узлы\n",
    "    \"eliminate_identity\",       # Убирает операции типа Identity\n",
    "    \"eliminate_nop_transpose\",  # Убирает ненужные транспонирования\n",
    "    \"fuse_bn_into_conv\",        # Сливает BatchNorm в Conv2d\n",
    "    \"fuse_add_bias_into_conv\"   # Сливает Add в Conv2d\n",
    "]\n",
    "\n",
    "optimized_model = onnxoptimizer.optimize(onnx_model, passes)\n",
    "optimized_onnx_path = \"mnist_cnn_optimized.onnx\"\n",
    "\n",
    "onnx.save(optimized_model, optimized_onnx_path)\n",
    "\n",
    "ort_session = ort.InferenceSession(optimized_onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"Доступные бэкенды:\", ort.get_available_providers())\n",
    "\n",
    "import numpy as np\n",
    "x = np.random.randn(1, 1, 28, 28).astype(np.float32)  # MNIST вход\n",
    "\n",
    "# Выполняем инференс\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: x}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "ort_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Разработка микросервиса предобработки данных**\n",
    "   - Спроектируйте микросервис для предобработки данных, необходимых для работы вашей модели.\n",
    "   - Реализуйте сервис с использованием **Flask**, **FastAPI** или другого подходящего фреймворка.\n",
    "   - Оформите микросервис в виде **Docker-контейнера**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: pillow in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from fastapi) (4.12.2)\n",
      "Collecting starlette<0.47.0,>=0.40.0\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting click>=7.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: colorama in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting anyio<5,>=3.6.2\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Collecting idna>=2.8\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.2)\n",
      "Installing collected packages: sniffio, idna, pydantic-core, anyio, annotated-types, starlette, pydantic, h11, click, uvicorn, fastapi\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.8.0 click-8.1.8 fastapi-0.115.11 h11-0.14.0 idna-3.10 pydantic-2.10.6 pydantic-core-2.27.2 sniffio-1.3.1 starlette-0.46.1 uvicorn-0.34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\GitHub\\HSE\\dev_ops\\hw1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "! pip install fastapi uvicorn pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Развёртывание моделей с помощью Triton Inference Server**\n",
    "   - Запустите **Triton Inference Server**.\n",
    "   - Задеплойте следующие версии модели:\n",
    "     - Оригинальная модель.\n",
    "     - Оптимизированная модель (на базе Torch/TensorFlow).\n",
    "     - Модель в формате ONNX.\n",
    "     - Оптимизированная модель ONNX.\n",
    "     - (Опционально) Модель в формате TRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker pull nvcr.io/nvidia/tritonserver:23.10-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 -v \"F:/GITHUB/HSE/DEV_OPS/HW1/TRITON_INFERENCE_SERVER/models:/models\" nvcr.io/nvidia/tritonserver:23.10-py3 tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Настройка мониторинга метрик**\n",
    "   - Настройте сбор и визуализацию метрик с помощью **Grafana** и **Prometheus**.\n",
    "   - Обеспечьте мониторинг как для Triton Inference Server, так и для микросервиса предобработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7080efbc3e39b5cc57ec21f780b4b94ca9ce9a66c2e4e129e726144e9c5b02ae\n"
     ]
    }
   ],
   "source": [
    "! docker run -d -p 9090:9090 -v \"F:\\GitHub\\HSE\\dev_ops\\hw1\\prometheus\\prometheus.yml:/etc/prometheus/prometheus.yml\" prom/prometheus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://host.docker.internal:9090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112e9de5f4e115e6d6f60581670cf7584666f1f5d6f63ca8b1b29cad2bab51d0\n"
     ]
    }
   ],
   "source": [
    "! docker run -d -p 3000:3000 --name grafana grafana/grafana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prometheus-fastapi-instrumentator\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting prometheus-client<1.0.0,>=0.8.0 (from prometheus-fastapi-instrumentator)\n",
      "  Downloading prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: starlette<1.0.0,>=0.30.0 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from prometheus-fastapi-instrumentator) (0.46.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from starlette<1.0.0,>=0.30.0->prometheus-fastapi-instrumentator) (4.8.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<1.0.0,>=0.30.0->prometheus-fastapi-instrumentator) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<1.0.0,>=0.30.0->prometheus-fastapi-instrumentator) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<1.0.0,>=0.30.0->prometheus-fastapi-instrumentator) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in f:\\github\\hse\\dev_ops\\hw1\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<1.0.0,>=0.30.0->prometheus-fastapi-instrumentator) (4.12.2)\n",
      "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
      "Installing collected packages: prometheus-client, prometheus-fastapi-instrumentator\n",
      "Successfully installed prometheus-client-0.21.1 prometheus-fastapi-instrumentator-7.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install prometheus-fastapi-instrumentator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **Оркестрация сервисов с помощью docker-compose**\n",
    "   - Поднимите весь стек сервисов через **docker-compose**, включающий:\n",
    "     - Микросервис предобработки данных.\n",
    "     - Triton Inference Server.\n",
    "     - Систему мониторинга (Prometheus и Grafana)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Тестирование и формирование отчёта**\n",
    "    - Проведите тестирование всех версий модели, отправляя запросы на инференс.\n",
    "    - Соберите логи работы сервисов.\n",
    "    - Сформируйте итоговый файл с отчётом, включающим результаты тестирования, собранные логи и анализ производительности."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
