services:

  preprocess:
    build:
      context: ./preprocess_service
    ports:
      - "8500:8500"
    expose:
      - "8500"
    networks:
      - triton_net

  triton:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    command: tritonserver --model-repository=/models
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ./triton_inference_server/models:/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    networks:
      - triton_net

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - triton_net

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    networks:
      - triton_net

networks:
  triton_net:
    driver: bridge
