{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "LLM для генерации текста\n",
    "Цель: Ознакомиться с использованием языковых моделей для генерации текста.\n",
    "\n",
    "Описание задачи:\n",
    "Напишите код для генерации текстов с использованием предобученной языковой модели Hugging Face, например, GPT-2 или другой подходящей модели. Генерируйте текст по заданной теме, например, \"Прогноз погоды\", \"Советы по фитнесу\" или \"История из будущего\".\n",
    "\n",
    "Ключевые шаги:\n",
    "\n",
    "Установите и настройте библиотеку Hugging Face Transformers.\n",
    "Загрузите предобученную модель (например, GPT-2).\n",
    "Напишите функцию для генерации текста на основе текстового префикса.\n",
    "Проверьте работу модели с разными параметрами (например, длина текста, температура, топ-k).\n",
    "Сравните результаты для разных начальных префиксов.\n",
    "\n",
    "Базово - GPT2, но лучше посмотреть LLama или Mistral, которую вы сможете запустить в коллабе (скорее всего не выше 8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator():\n",
    "    def __init__(self, model, tokenizer, device = 'cpu'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt,\n",
    "        max_length=100, \n",
    "        temperature=1.0, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=1\n",
    "    ):\n",
    "        outputs = self.model.generate(\n",
    "            self.tokenizer.encode(prompt, return_tensors='pt').to(self.device),\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id       \n",
    "        )\n",
    "\n",
    "        return [\n",
    "            self.tokenizer.decode(\n",
    "                output, \n",
    "                skip_special_tokens=True\n",
    "            ).strip() for output in outputs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Параметр**            | **Описание**                                              | **Пример**           |\n",
    "|--------------------------|----------------------------------------------------------|---------------------------------|\n",
    "| `prompt`                | Начальный текст для генерации.                           | Строка (например, `\"Hello AI\"`) |\n",
    "| `max_length`            | Максимальная длина генерируемого текста (в токенах).     | `50–200`                       |\n",
    "| `temperature`           | Управляет случайностью выбора слов.                     | `0.1–1.5`                      |\n",
    "| `top_k`                 | Ограничивает выбор **k самых вероятных слов**.           | `50` (для разнообразия)         |\n",
    "| `top_p`                 | Ограничивает выбор **по сумме вероятностей**.           | `0.9` (nucleus sampling)        |\n",
    "| `num_return_sequences`  | Количество сгенерированных текстов на один `prompt`.     | `1–5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.utils\n",
    "\n",
    "text_generator = TextGenerator(\n",
    "    model=GPT2LMHeadModel.from_pretrained('gpt2'), \n",
    "    tokenizer=GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The weather today\",\n",
    "    \"ChatGPT is\",\n",
    "    \"The world in 2050\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Style\n",
    "\n",
    "def generate_and_print_texts(\n",
    "        prompts,\n",
    "        max_length=100, \n",
    "        temperature=1.0, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=1\n",
    "    ):\n",
    "    for prompt in prompts:\n",
    "        print(Fore.CYAN + Style.BRIGHT + f\"\\n--- Prompt ---\" + Style.RESET_ALL)\n",
    "        print(Fore.YELLOW + f\"{prompt}\" + Style.RESET_ALL)\n",
    "\n",
    "        texts = text_generator.generate(\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            num_return_sequences=num_return_sequences\n",
    "        )\n",
    "        \n",
    "        print(Fore.CYAN + Style.BRIGHT + \"\\nGenerated Texts:\" + Style.RESET_ALL)\n",
    "        for idx, text in enumerate(texts, 1):\n",
    "            print(Fore.GREEN + f\"[{idx}] \" + Style.RESET_ALL + f\"{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mThe weather today\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mThe weather today could be a little warmer than normal.\n",
      "\n",
      "\"If you were going back around, it would be about 7 degrees F in the middle of day tomorrow, but it will be slightly lower then yesterday because the wind is going to be colder,\" said Dr Chris Waddell of West Yorkshire's University of Wrexham.\n",
      "\n",
      "He added: \"People are thinking 'What the heck is going on?'\"\n",
      "\n",
      "Earlier today it was reported a train leaving Glasgow was halted because passengers\n",
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mChatGPT is\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mChatGPT is in the works at Mozilla with a plan to put together its own website. In order to create that, Mozilla is in the process of developing its own \"Firefox Extension Service.\"\n",
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mThe world in 2050\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mThe world in 2050, around 300% more people live in countries with the highest share of deforestation. At the same time, the Earth needs climate action more than ever. Even as the carbon dioxide levels rise, the planet is losing more and more heat, and we are losing more and more coral reefs as a result of climate change.\n",
      "\n",
      "By 2050, if the planet is carbon intensive, there will be less carbon dioxide in the atmosphere than when carbon dioxide levels were so low and we did not\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_texts(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mThe weather today\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mThe weather today wasn't too much nicer this afternoon so don't be shy – be sure!\n",
      "\n",
      "The local bike path along to and along on one of two bike paths has the nice chance to become 'Wendoc'. To walk over both trails start at St Peter to find their starting point. Walking over St Peter also makes great use of 'Bicycycle Lanes (LMB). I didn't spend much time in Cattlin for this ride today and decided to use it for\n",
      "\u001b[32m[2] \u001b[0mThe weather today turned quite bad today for some of our crew here at Base Brest, where in winter we'd try to hold off rain so weather isn't bad too, though that probably wont hurt much either here but it has put a strain on some cool conditions. The morning and end of each day's runs will end there as expected!\n",
      " All weekend this past evening this is so pretty and my whole life has seemed normal and perfect as you walk through our gardens. It can always be\n",
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mChatGPT is\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mChatGPT is probably more like how to deal the rest of its internal community problems (such as \"Why is there a ton more of white supremacists out here?\") then you will know what can be improved, what could benefit it now. And also, as someone saying this the other week \"Hey Google is better off and they probably already hate people here than any person for all other reason that anyone who doesn't hate has heard of.\" The thing about Facebook – is that while everybody is working\n",
      "\u001b[32m[2] \u001b[0mChatGPT is here with a different game in focus every week. We got together this Friday to chat about gaming, the industry, and its issues as people go from AAA to major hits such as Gears of War 4.\n",
      "\n",
      "With Gears 2, an AAA masterpiece this year—a huge disappointment—many people seem to agree Sony, but have been wrong since 2012 to acknowledge them as having a strong voice with consumers of video games.\n",
      "\n",
      "The PlayStation 4 and Xbox One game industry remains dominated\n",
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mThe world in 2050\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mThe world in 2050 may be over 300 places away at the end of its long road away [and more.] To reach such a close for all humanity has some monumental challenges ahead,'' [pg. 584] The U.S. Census in 2009 estimated worldwide, at 3.4 millon per million to be over two millions. According to the Harvard Department of Geostatistics;\n",
      "It would take another 5,000 decades... but a time and world revolution at 1 million kilometers [4\n",
      "\u001b[32m[2] \u001b[0mThe world in 2050 may rise, in that by 2100 more people will die in a country for which all existing deaths are attributed to people running in large urbanities. These death counts alone are an additional 100,00 as we take account that urban deaths occur around three to 10 per thousand. This adds only about 15 years to human activity that would end there. Yet all we ask is not 'What percentage of that future generation you will never ever live together; just that a greater chance to kill yourself\n"
     ]
    }
   ],
   "source": [
    "# Повысим температуру\n",
    "generate_and_print_texts(prompts=prompts, temperature=2.0, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mThe weather today\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mThe weather today is very good, but I'm not sure if I'll be able to get to the airport tomorrow. I'm not sure if I'll be able to get to the airport tomorrow.\n",
      "\n",
      "I'm not sure if I'll be able to get to the airport tomorrow. I'm not sure if I'll be able to get to the airport tomorrow.\n",
      "\n",
      "I'm not sure if I'll be able to get to the airport tomorrow.\n",
      "\n",
      "I'm not sure if I\n",
      "\u001b[32m[2] \u001b[0mThe weather today was very good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was good. The weather was\n",
      "\u001b[32m[3] \u001b[0mThe weather today was very good. I was able to get a good view of the area and the area around the lake. I was able to get a good view of the area and the area around the lake. I was able to get a good view of the area and the area around the lake. I was able to get a good view of the area and the area around the lake. I was able to get a good view of the area and the area around the lake. I was able\n",
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mChatGPT is\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mChatGPT is a free, open source, open-source, open-source, distributed application framework for the Java programming language. It is designed to be a simple, fast, and secure Java application framework.\n",
      "\n",
      "The project is based on the Java programming language. It is based on the Java programming language.\n",
      "\n",
      "The project is based on the Java programming language. It is based on the Java programming language.\n",
      "\n",
      "The project is based on the Java programming language. It is based\n",
      "\u001b[32m[2] \u001b[0mChatGPT is a free service that allows you to create and share your own GPT files.\n",
      "\n",
      "You can also create your own GPT files with the following command:\n",
      "\n",
      "GPTGPT -P /path/to/your/gpt/file.gpt\n",
      "\n",
      "You can also create your own GPT files with the following command:\n",
      "\n",
      "GPTGPT -P /path/to/your/gpt/file.gpt\n",
      "\n",
      "You can\n",
      "\u001b[32m[3] \u001b[0mChatGPT is a free service that allows you to send and receive emails from your Gmail account.\n",
      "\n",
      "If you're using Gmail, you can also use it to send and receive emails from your Gmail account.\n",
      "\n",
      "You can also use Gmail to send and receive emails from your Gmail account.\n",
      "\n",
      "You can also use Gmail to send and receive emails from your Gmail account.\n",
      "\n",
      "You can also use Gmail to send and receive emails from your Gmail account.\n",
      "\n",
      "You can also use\n",
      "\u001b[36m\u001b[1m\n",
      "--- Prompt ---\u001b[0m\n",
      "\u001b[33mThe world in 2050\u001b[0m\n",
      "\u001b[36m\u001b[1m\n",
      "Generated Texts:\u001b[0m\n",
      "\u001b[32m[1] \u001b[0mThe world in 2050 is going to be a lot more diverse than it was in the past.\n",
      "\n",
      "\"We're going to have a lot more people who are going to be able to live in cities, and we're going to have a lot more people who are going to be able to live in cities, and we're going to have a lot more people who are going to be able to live in cities, and we're going to have a lot more people who are going to be able\n",
      "\u001b[32m[2] \u001b[0mThe world in 2050 will be the most populous on Earth, according to the UN's World Population Prospects.\n",
      "\n",
      "The UN's World Population Prospects report, released in September, said that the world's population will reach 1.3 billion by 2050, up from 1.6 billion in 2010.\n",
      "\n",
      "The report said that the world's population will reach 1.3 billion by 2050, up from 1.6 billion in 2010.\n",
      "\n",
      "The report said that the world's population will\n",
      "\u001b[32m[3] \u001b[0mThe world in 2050 will be a world in which the world's population will grow by about 2.5 billion people, according to the UN.\n",
      "\n",
      "The UN's Population Division said the world's population will grow by about 2.5 billion people by 2050.\n",
      "\n",
      "The world's population will grow by about 2.5 billion people by 2050.\n",
      "\n",
      "The world's population will grow by about 2.5 billion people by 2050.\n",
      "\n",
      "The world's population will grow by about 2\n"
     ]
    }
   ],
   "source": [
    "# Теперь попробуем с низкой температурой\n",
    "generate_and_print_texts(prompts=prompts, temperature=0.2, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить, при более высокой температуре, модель выдает разнообразные текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоги:\n",
    "\n",
    "Таким образом, можно выделить несколько комбинаций параметров:\n",
    "\n",
    "1. **Строгий, детерминированный вывод (максимальная точность)**:\n",
    "```python\n",
    "do_sample=False, temperature=0.0\n",
    "```\n",
    "\n",
    "2. **Творческий и разнообразный текст:**\n",
    "```python\n",
    "do_sample=True, temperature=0.8, top_p=0.9\n",
    "```\n",
    "\n",
    "3. **Баланс между качеством и разнообразием:**\n",
    "```python\n",
    "do_sample=True, temperature=0.7, top_k=50\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "Классификация текста с BERT\n",
    "Цель: Понять, как использовать предобученные модели типа BERT для задач классификации текста.\n",
    "\n",
    "Описание задачи:\n",
    "Реализуйте модель классификации текстов на основе предобученной модели BERT. Используйте датасет (IMDb Reviews) https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews  для классификации отзывов на положительные и отрицательные, оцените модель и дайте выводы. Можно модель дообучать или использовать зиро шот, на ваш выбор, главное добиться нужного качества\n",
    "\n",
    "Дополнительные баллы (взамен задания 2): выбрать модель на русском языке и попробовать классифицировать запрос (нужно отправить фото или нет) -- например \"скинь фото\" = 1, \"как дела?\" = 0, здесь задача сиро-шот, нужно найти правильно модель для русского, добавьте в бук примеров вызова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование меток в числовой формат\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"sentiment\"] = label_encoder.fit_transform(data[\"sentiment\"])  # positive -> 1, negative -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение на тренировочные и тестовые выборки\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data[\"review\"].tolist(), data[\"sentiment\"].tolist(), test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Инициализация токенизатора и модели\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кастомный класс Dataset\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataLoader\n",
    "train_dataset = IMDBDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = IMDBDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оптимизатор и планировщик\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подключение к GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "for layer in model.bert.encoder.layer[:6]:  # Заморозка первых 6 слоёв\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 2500/2500 [11:55<00:00,  3.50it/s, Loss=0.218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 2500/2500 [11:56<00:00,  3.49it/s, Loss=0.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 2500/2500 [11:56<00:00,  3.49it/s, Loss=0.0535]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Обучение модели\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.set_postfix({\"Loss\": total_loss / len(progress_bar)})\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9437\n"
     ]
    }
   ],
   "source": [
    "# Оценка модели\n",
    "model.eval()\n",
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        accuracy += (predictions == batch[\"labels\"]).sum().item()\n",
    "\n",
    "accuracy = accuracy / len(test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_imdb\\\\tokenizer_config.json',\n",
       " './bert_imdb\\\\special_tokens_map.json',\n",
       " './bert_imdb\\\\vocab.txt',\n",
       " './bert_imdb\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохранение дообученной модели\n",
    "model.save_pretrained(\"./bert_imdb\")\n",
    "tokenizer.save_pretrained(\"./bert_imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования обученной модели для произвольного текста\n",
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    sentiment = \"positive\" if prediction == 1 else \"negative\"\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Пример вызова функции с текстом\n",
    "example_text = \"This movie was absolutely fantastic! The plot was engaging and the characters were well-developed.\"\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(\"./bert_imdb\").to(device)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(\"./bert_imdb\")\n",
    "result = predict_sentiment(example_text, loaded_model, loaded_tokenizer, device)\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Пример вызова функции с текстом\n",
    "example_text = \"I had high expectations, but this film was a letdown. The pacing was uneven, and the plot felt overly complicated and dragged out.\"\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(\"./bert_imdb\").to(device)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(\"./bert_imdb\")\n",
    "result = predict_sentiment(example_text, loaded_model, loaded_tokenizer, device)\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
