## Домашнее задание 3: Обучение LLaMA

Мягкий дедлайн -- неделя с момента получения (5 дек 2024) 

Задание 1: LLM для генерации текста
Цель: Ознакомиться с использованием языковых моделей для генерации текста.

Описание задачи:
Напишите код для генерации текстов с использованием предобученной языковой модели Hugging Face, например, GPT-2 или другой подходящей модели. Генерируйте текст по заданной теме, например, "Прогноз погоды", "Советы по фитнесу" или "История из будущего".

Ключевые шаги:

Установите и настройте библиотеку Hugging Face Transformers.
Загрузите предобученную модель (например, GPT-2).
Напишите функцию для генерации текста на основе текстового префикса.
Проверьте работу модели с разными параметрами (например, длина текста, температура, топ-k).
Сравните результаты для разных начальных префиксов.

Базово - GPT2, но лучше посмотреть LLama или Mistral, которую вы сможете запустить в коллабе (скорее всего не выше 8B) 


Задание 2: Классификация текста с BERT
Цель: Понять, как использовать предобученные модели типа BERT для задач классификации текста.

Описание задачи:
Реализуйте модель классификации текстов на основе предобученной модели BERT. Используйте датасет (IMDb Reviews) https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews  для классификации отзывов на положительные и отрицательные, оцените модель и дайте выводы. Можно модель дообучать или использовать зиро шот, на ваш выбор, главное добиться нужного качества 

Дополнительные баллы (взамен задания 2): выбрать модель на русском языке и попробовать классифицировать запрос (нужно отправить фото или нет) -- например "скинь фото" = 1, "как дела?" = 0, здесь задача сиро-шот, нужно найти правильно модель для русского, добавьте в бук примеров вызова